# Clara
## Privacy-First AI Assistant & App Builder that has no Docker or Backend requirement.

<div align="center">

<!-- link to clara access and we promise no data is sent anywhere other than your pc  https://clara-ollama.netlify.app/ just the link to the app-->

### ğŸš€ Live Demo, 
Link to the Clara App: [Clara](https://clara-ollama.netlify.app/)

[![Clara](https://img.shields.io/badge/Clara-0.1.2-FFD700.svg)](https://clara-ollama.netlify.app/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.5.3-blue.svg)](https://www.typescriptlang.org/)
[![React](https://img.shields.io/badge/React-18.3.1-blue.svg)](https://reactjs.org/)
[![Vite](https://img.shields.io/badge/Vite-5.4.2-646CFF.svg)](https://vitejs.dev/)
[![Tailwind CSS](https://img.shields.io/badge/Tailwind_CSS-3.4.1-38B2AC.svg)](https://tailwindcss.com/)
[![Electron](https://img.shields.io/badge/Electron-35.0.1-47848F.svg)](https://www.electronjs.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Your local AI assistant that respects your privacy**

</div>

## ğŸ”’ Privacy First, No Exceptions

Clara is a completely client-side AI assistant that works with Ollama running on your machine or network. Unlike cloud-based solutions:

- **No data leaves your device** without your explicit permission
- **Zero tracking or telemetry**
- **All data stored locally** in your browser's IndexedDB
- **Direct connection** to your Ollama instance

## âœ¨ Features

### Chat Interface
- ğŸ’¬ Real-time chat with streaming responses
- ğŸ¤– Support for all Ollama models (Llama, Mistral, Phi, etc.)
- ğŸ–¼ï¸ Image understanding with multimodal models
- ğŸ’¾ Persistent conversation history stored locally
- ğŸ“ Rich markdown support with code highlighting

### App Builder
- ğŸ§© Visual node-based flow builder
- ğŸ“Š Text input/output nodes
- ğŸ–¼ï¸ Image input nodes
- ğŸ¤– LLM integration nodes
- âš¡ Conditional logic nodes
- ğŸŒ API call nodes
- âœï¸ Text combiner nodes
- ğŸ”„ Reusable app templates
- ğŸ’» Run apps with user inputs

### Desktop Application
- ğŸ–¥ï¸ Native desktop experience with Electron
- ğŸ”„ Same features as the web version
- ğŸš€ Better performance for resource-intensive tasks
- ğŸ’» Cross-platform support (Windows, macOS, Linux)
- ğŸ”Œ Enhanced system integration
- ğŸ”’ Local-first approach for maximum privacy

### System
- ğŸŒ“ Beautiful light/dark mode
- ğŸ” Model management and selection
- ğŸ“± Responsive design
- ğŸ› ï¸ Custom API configurations

## ğŸ”® Coming Soon
- ğŸ¨ Image generation with Stable Diffusion
- ğŸ”Š Voice input and output
- ğŸ‘¥ Character personalities for chat
- ğŸ“š Knowledge base integration 
- ğŸ“Š Data visualization nodes
- ğŸ“„ PDF document processing
- ğŸ”Œ Plugin system for extensibility
- ğŸ“± PWA for mobile installation
- ğŸš€ Local RAG with vector databases

## ğŸš€ Getting Started

### Prerequisites
- **Node.js** (v18+ recommended)
- **Ollama** installed locally ([install instructions below](#installing-ollama))

### Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/clara-ai.git
cd clara-ai

# Install dependencies
npm install

# Start development server (web version)
npm run dev

# Start development server (desktop version)
npm run electron:dev
```

### Building the Desktop App
```bash
# Build for production (web)
npm run build

# Build desktop application
npm run electron:build
```

The desktop application will be created in the `release` directory, with installers for your current platform.

> **Note:** The `release` directory and other Electron build artifacts are ignored in the repository via `.gitignore`. This includes installer files (`.dmg`, `.exe`, `.deb`, `.AppImage`), update files (`.blockmap`, `latest-*.yml`), and temporary build directories.

## ğŸ³ Installing Ollama

### Windows
1. Download the installer from the [official site](https://ollama.ai/download/windows)
2. Run the installer and follow the prompts
3. After installation, Ollama will be available at `http://localhost:11434`
4. Enable CORS by creating a file named `config.json` in `%USERPROFILE%\.ollama` with:
   ```json
   {
     "origins": ["*"]
   }
   ```

### macOS
1. Download Ollama from the [official site](https://ollama.ai/download/mac)
2. Install the application
3. Run Ollama from Applications folder
4. Ollama will be available at `http://localhost:11434`
5. Enable CORS by creating or editing `~/.ollama/config.json`:
   ```json
   {
     "origins": ["*"]
   }
   ```

### Linux
1. Install Ollama using the command:
   ```bash
   curl -fsSL https://ollama.ai/install.sh | sh
   ```
2. Start Ollama:
   ```bash
   ollama serve
   ```
3. Enable CORS by creating or editing `~/.ollama/config.json`:
   ```json
   {
     "origins": ["*"]
   }
   ```

### Pulling Models
Once Ollama is running, you can pull models:
```bash
# Pull a basic model
ollama pull llama3

# Pull multimodal (image understanding) model
ollama pull llava
```

## ğŸŒ Remote Access Options

### Using Ollama on Another Computer
If you're running Ollama on another computer on your network, just enter the IP address in Clara's settings: `http://{IP_ADDRESS}:11434`

### Using ngrok for Remote Access
To access Ollama from anywhere:

```bash
# Install ngrok
npm install -g ngrok

# Expose Ollama API
ngrok http 11434
```

Then use the provided ngrok URL in Clara's settings.

## ğŸ—ï¸ Project Architecture
Clara is built with a modular architecture:
```
clara/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # UI components
â”‚   â”œâ”€â”€ hooks/          # Custom hooks
â”‚   â”œâ”€â”€ utils/          # Helper functions
â”‚   â”œâ”€â”€ db/             # Local storage (IndexedDB)
â”‚   â”œâ”€â”€ types/          # TypeScript type definitions
â”‚   â””â”€â”€ App.tsx         # Application entry
â”œâ”€â”€ electron/           # Electron-specific code
â”‚   â”œâ”€â”€ main.cjs        # Main process
â”‚   â””â”€â”€ preload.cjs     # Preload script
â”œâ”€â”€ public/             # Static assets
â””â”€â”€ package.json        # Dependencies
```

## ğŸš¢ Deployment
- **Web Version**: Deploy the `dist` directory to any static host (e.g., Netlify, GitHub Pages).
- **Desktop Version**: Use `npm run electron:build` to create installers for Windows, macOS, and Linux.

## ğŸ¤ Contribute
1. Fork repository
2. Create feature branch (`git checkout -b feature/YourFeature`)
3. Commit changes (`git commit -m 'Add YourFeature'`)
4. Push branch (`git push origin feature/YourFeature`)
5. Submit Pull Request

## ğŸ“„ License
MIT License â€“ [LICENSE](LICENSE)

---

ğŸŒŸ **Built with privacy and security at its core.** ğŸŒŸ